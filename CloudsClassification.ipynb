{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXzlhIrSeW/ubY7c1oDOg1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samugatu/CIS-IEEE/blob/main/CloudsClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classificação de Nuvens usando CNN"
      ],
      "metadata": {
        "id": "VZ8pcEDJkdn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# part 1: Desafio Kaggle (Rede do Zero com pytorch e tochvision)"
      ],
      "metadata": {
        "id": "PqIM-VgZ0v8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objetivo**: Contruir e treinar uma rede neural simples e obter uma boa acurácia na classificação."
      ],
      "metadata": {
        "id": "vpWIGtoxkkGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DiAskWalaOMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bibliotecas utilizadas:"
      ],
      "metadata": {
        "id": "-G7_WTqZaVhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision import models\n",
        "import os\n",
        "from torchvision.datasets import ImageFolder\n",
        "import zipfile\n"
      ],
      "metadata": {
        "id": "21b66FCwYKMf"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definindo as trasnformações:"
      ],
      "metadata": {
        "id": "IToKsllPaY3s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "LT2i8-q8j8F7"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos carregar nosso dataset de treino e test:"
      ],
      "metadata": {
        "id": "tjEZTC19auq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile(\"clouds.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"clouds\")"
      ],
      "metadata": {
        "id": "KTpKOGClLMTZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_dir = '/content/clouds/clouds/clouds_train'\n",
        "test_dir = '/content/clouds/clouds/clouds_test'\n",
        "\n",
        "train_dataset = ImageFolder(train_dir, transform=transform)\n",
        "test_dataset = ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "CDQYagIiaybZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos definir nossa rede neural, com duas camadas"
      ],
      "metadata": {
        "id": "mJ_Z8QYrMUwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, numclasses):\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.pool1 = nn.MaxPool2d(2, 2)\n",
        "    self.fc1 = nn.Linear(16 * 16 * 16, numclasses)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=self.pool1(torch.relu(self.conv1(x)))\n",
        "    x=x.view(-1, 16 * 16 * 16)\n",
        "    x=self.fc1(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "9Ll3EAWjMaRe"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos incializar o modelo:"
      ],
      "metadata": {
        "id": "H7ClICUuNjd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numclasses = len(train_dataset.classes)\n",
        "model = Net(numclasses=numclasses)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "xBj8gFplczUA"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos para o treinamento:"
      ],
      "metadata": {
        "id": "wu_A0hyJOiVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 25\n",
        "\n",
        "for epochs in range(num_epochs):\n",
        "  running_loss = 0.0\n",
        "  for inputs, labels in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  epoch_loss = running_loss / len(train_loader)\n",
        "  print(f'Epoch {epochs+1}/{num_epochs}, Loss: {epoch_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sTYWX5SOmkw",
        "outputId": "405ae35b-cc19-4421-f33b-f91185e1ac6c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25, Loss: 1.6974\n",
            "Epoch 2/25, Loss: 1.4237\n",
            "Epoch 3/25, Loss: 1.2639\n",
            "Epoch 4/25, Loss: 1.1533\n",
            "Epoch 5/25, Loss: 1.0989\n",
            "Epoch 6/25, Loss: 1.0502\n",
            "Epoch 7/25, Loss: 1.0268\n",
            "Epoch 8/25, Loss: 0.9727\n",
            "Epoch 9/25, Loss: 0.8921\n",
            "Epoch 10/25, Loss: 0.8659\n",
            "Epoch 11/25, Loss: 0.8603\n",
            "Epoch 12/25, Loss: 0.8047\n",
            "Epoch 13/25, Loss: 0.7874\n",
            "Epoch 14/25, Loss: 0.7655\n",
            "Epoch 15/25, Loss: 0.7129\n",
            "Epoch 16/25, Loss: 0.6813\n",
            "Epoch 17/25, Loss: 0.6778\n",
            "Epoch 18/25, Loss: 0.6852\n",
            "Epoch 19/25, Loss: 0.6259\n",
            "Epoch 20/25, Loss: 0.6079\n",
            "Epoch 21/25, Loss: 0.6052\n",
            "Epoch 22/25, Loss: 0.5729\n",
            "Epoch 23/25, Loss: 0.5581\n",
            "Epoch 24/25, Loss: 0.5396\n",
            "Epoch 25/25, Loss: 0.5210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos definir uma função de avaliação do modelo:"
      ],
      "metadata": {
        "id": "xedHqEKLP9Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in dataloader:\n",
        "      outputs = model(inputs)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  accuracy = 100 * correct / total\n",
        "  print(f'Accuracy: {accuracy:.2f}%')\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "0VvQTWlSP8Tr"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos avaliar nos arquivos test:"
      ],
      "metadata": {
        "id": "ACH8kJXJPc5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy1 = evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2_Nf_snPj4R",
        "outputId": "d7a92fec-6505-42e3-b02d-aae287f686f2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 55.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este resultado não esta satisfatório, vamos tentar fazer algumas mudanças, como adicionar mais uma camada covulacional."
      ],
      "metadata": {
        "id": "mvS2l2rXRlLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net2(nn.Module):\n",
        "  def __init__(self, numclasses):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    self.fc1 = nn.Linear(32 * 8 * 8, numclasses)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=self.pool1(torch.relu(self.conv1(x)))\n",
        "    x=self.pool2(torch.relu(self.conv2(x)))\n",
        "    x=x.view(-1, 32 * 8 * 8)\n",
        "    x=self.fc1(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "bXMI8smcR8V7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicializar a segunda rede:"
      ],
      "metadata": {
        "id": "VfUktDTvSiA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numclasses = len(train_dataset.classes)\n",
        "model2 = Net2(numclasses=numclasses)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model2.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "7qYQXl_ASl3p"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora o treinamento e a avaliação:"
      ],
      "metadata": {
        "id": "BAq3bot9TNVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "\n",
        "for epochs in range(num_epochs):\n",
        "  running_loss = 0.0\n",
        "  for inputs, labels in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model2(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  epoch_loss = running_loss / len(train_loader)\n",
        "  print(f'Epoch {epochs+1}/{num_epochs}, Loss: {epoch_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rv72W6xTQfS",
        "outputId": "aa68ac53-5fff-4e6c-ea34-0780453ba5cb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 1.8550\n",
            "Epoch 2/50, Loss: 1.6948\n",
            "Epoch 3/50, Loss: 1.5352\n",
            "Epoch 4/50, Loss: 1.3733\n",
            "Epoch 5/50, Loss: 1.2418\n",
            "Epoch 6/50, Loss: 1.1810\n",
            "Epoch 7/50, Loss: 1.1126\n",
            "Epoch 8/50, Loss: 1.0946\n",
            "Epoch 9/50, Loss: 1.0294\n",
            "Epoch 10/50, Loss: 1.0522\n",
            "Epoch 11/50, Loss: 1.0166\n",
            "Epoch 12/50, Loss: 0.9440\n",
            "Epoch 13/50, Loss: 0.9091\n",
            "Epoch 14/50, Loss: 0.8906\n",
            "Epoch 15/50, Loss: 0.8747\n",
            "Epoch 16/50, Loss: 0.8505\n",
            "Epoch 17/50, Loss: 0.8104\n",
            "Epoch 18/50, Loss: 0.7544\n",
            "Epoch 19/50, Loss: 0.7270\n",
            "Epoch 20/50, Loss: 0.7023\n",
            "Epoch 21/50, Loss: 0.6630\n",
            "Epoch 22/50, Loss: 0.6444\n",
            "Epoch 23/50, Loss: 0.6087\n",
            "Epoch 24/50, Loss: 0.5965\n",
            "Epoch 25/50, Loss: 0.5825\n",
            "Epoch 26/50, Loss: 0.5609\n",
            "Epoch 27/50, Loss: 0.5407\n",
            "Epoch 28/50, Loss: 0.5153\n",
            "Epoch 29/50, Loss: 0.5443\n",
            "Epoch 30/50, Loss: 0.4736\n",
            "Epoch 31/50, Loss: 0.4677\n",
            "Epoch 32/50, Loss: 0.4406\n",
            "Epoch 33/50, Loss: 0.4448\n",
            "Epoch 34/50, Loss: 0.4026\n",
            "Epoch 35/50, Loss: 0.3944\n",
            "Epoch 36/50, Loss: 0.3956\n",
            "Epoch 37/50, Loss: 0.3791\n",
            "Epoch 38/50, Loss: 0.3703\n",
            "Epoch 39/50, Loss: 0.3809\n",
            "Epoch 40/50, Loss: 0.3655\n",
            "Epoch 41/50, Loss: 0.3587\n",
            "Epoch 42/50, Loss: 0.3694\n",
            "Epoch 43/50, Loss: 0.3168\n",
            "Epoch 44/50, Loss: 0.2931\n",
            "Epoch 45/50, Loss: 0.3063\n",
            "Epoch 46/50, Loss: 0.3214\n",
            "Epoch 47/50, Loss: 0.2817\n",
            "Epoch 48/50, Loss: 0.2888\n",
            "Epoch 49/50, Loss: 0.2519\n",
            "Epoch 50/50, Loss: 0.2488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy2 = evaluate_model(model2, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLw-z00DUBW7",
        "outputId": "4a68d313-3ab1-4d13-9ddf-7c35b09360e0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 64.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Acurácia modelo 1: {accuracy1:.2f}%')\n",
        "print(f'Acurácia modelo 2: {accuracy2:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDOPWlRiWlyL",
        "outputId": "3bbd546b-e549-420a-ece6-b076b665ec65"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia modelo 1: 55.35%\n",
            "Acurácia modelo 2: 64.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos gerar nosso arquivo de submissão como pedido no desafio:"
      ],
      "metadata": {
        "id": "0ehdZ8y-wihu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "model2.eval()\n",
        "idx_to_class = {v: k for k, v in train_dataset.class_to_idx.items()}\n",
        "test_filenames = [os.path.basename(f[0]) for f in test_dataset.samples]\n",
        "\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "\n",
        "        predict_labels = [idx_to_class[idx.item()] for idx in predicted]\n",
        "        predictions.extend(predict_labels)\n",
        "\n",
        "submission_df = pd.DataFrame({'row_id': test_filenames, 'label': predictions})\n",
        "submission_df.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "id": "vK5faZ7CwsU4"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui foram feitas duas redes com configurações diferentes e foi obtido a acurácia de:\n",
        "\n",
        "Acurácia modelo 1: 59.88%\n",
        "\n",
        "\n",
        "Acurácia modelo 2: 64.40%\n",
        "\n",
        "Não é um bomr esultado, mas para uma rede do zero e bem simples é um resultado aceitável."
      ],
      "metadata": {
        "id": "dzYlODUg1BLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 2: Pegar um rede pré treianda"
      ],
      "metadata": {
        "id": "oatfSLxN1t0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_resnet = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "transform_train_resnet = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=transform_train_resnet)\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform=transform_resnet)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"Carregando o modelo ResNet-18 pré-treinado...\")\n",
        "model_transfer = models.resnet18(pretrained=True)\n",
        "\n",
        "for param in model_transfer.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_classes = len(train_dataset.classes)\n",
        "num_features = model_transfer.fc.in_features\n",
        "model_transfer.fc = nn.Linear(num_features, num_classes)\n",
        "\n",
        "\n",
        "model_transfer = model_transfer.to(device)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model_transfer.fc.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "print(\"Iniciando o treinamento (transfer learning)...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model_transfer.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_transfer(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "\n",
        "    model_transfer.eval()\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model_transfer(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Acurácia Teste: {accuracy:.2f}%')\n",
        "\n",
        "print(\"Treinamento concluído.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km4MnZCz42PS",
        "outputId": "39ddee14-22d3-4dcb-b560-b0fe7954ddef"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando o modelo ResNet-18 pré-treinado...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando o treinamento (transfer learning)...\n",
            "Epoch 1/10, Loss: 1.6795, Acurácia Teste: 58.23%\n",
            "Epoch 2/10, Loss: 1.0949, Acurácia Teste: 81.48%\n",
            "Epoch 3/10, Loss: 0.7943, Acurácia Teste: 81.28%\n",
            "Epoch 4/10, Loss: 0.6463, Acurácia Teste: 85.19%\n",
            "Epoch 5/10, Loss: 0.5093, Acurácia Teste: 87.24%\n",
            "Epoch 6/10, Loss: 0.4371, Acurácia Teste: 87.65%\n",
            "Epoch 7/10, Loss: 0.4052, Acurácia Teste: 88.89%\n",
            "Epoch 8/10, Loss: 0.3597, Acurácia Teste: 89.71%\n",
            "Epoch 9/10, Loss: 0.3372, Acurácia Teste: 89.71%\n",
            "Epoch 10/10, Loss: 0.3173, Acurácia Teste: 89.51%\n",
            "Treinamento concluído.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver aqui com uma rede pré-treinada a rede tem uma acurácia bem superior,\n",
        "além disso, podemos notar que a partir da sétima época , apesar da função perda diminuir, a acurácia também diminui, notando já um overfiting. Assim, vamos considerar 7 épocas o ideal."
      ],
      "metadata": {
        "id": "wfJjM30XXd7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 3: Regularização e Data agumentation"
      ],
      "metadata": {
        "id": "Ro7vOTiUYQpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos implentar na nossa rede neural as regularizações L2 e DropOut"
      ],
      "metadata": {
        "id": "hRuYwwV9aZuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = model_transfer.fc.in_features\n",
        "num_classes = len(train_dataset.classes)\n",
        "\n",
        "model_transfer.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, num_classes)\n",
        ")\n",
        "\n",
        "model_transfer = model_transfer.to(device)\n",
        "optimizer = optim.Adam(model_transfer.parameters(), lr=0.001, weight_decay=0.0001)"
      ],
      "metadata": {
        "id": "keU6JVBBaqBr"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for eacho in range(num_epochs):\n",
        "  model_transfer.train()\n",
        "  running_loss = 0.0\n",
        "  for inputs, labels in train_loader:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outputs = model_transfer(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  epoch_loss = running_loss / len(train_loader)\n",
        "\n",
        "  model_transfer.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model_transfer(inputs)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  accuracy = 100 * correct / total\n",
        "  print(f'Epoch {eacho+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Acurácia Teste: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv17ReJccUKP",
        "outputId": "0853b78c-9da8-48e2-f11f-b5c4cb65becd"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.5215, Acurácia Teste: 74.49%\n",
            "Epoch 2/10, Loss: 0.7690, Acurácia Teste: 84.36%\n",
            "Epoch 3/10, Loss: 0.5532, Acurácia Teste: 87.86%\n",
            "Epoch 4/10, Loss: 0.4979, Acurácia Teste: 83.54%\n",
            "Epoch 5/10, Loss: 0.4512, Acurácia Teste: 87.24%\n",
            "Epoch 6/10, Loss: 0.5121, Acurácia Teste: 88.89%\n",
            "Epoch 7/10, Loss: 0.2974, Acurácia Teste: 86.42%\n",
            "Epoch 8/10, Loss: 0.2836, Acurácia Teste: 89.92%\n",
            "Epoch 9/10, Loss: 0.2461, Acurácia Teste: 89.71%\n",
            "Epoch 10/10, Loss: 0.2233, Acurácia Teste: 89.51%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora ja temos uma acurácia de quse 90% utilizando uma rede pré treinada e alguns métodos de regularização. Agora para tentar aumentar ainda mais a acurácia iremos aplicar o Data Augmentation, na prática iremos gerar \"novas\" imagens baseadas nas ja existentes, fazendo rotações aleatórias, ajuste de cores etc. Assimteremos um conjunto de treino ainda maior"
      ],
      "metadata": {
        "id": "N9prLb6sgozx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S-yxLK_eusX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transform_train_resnet = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.TrivialAugmentWide(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "trnsform_test_resnet = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=transform_train_resnet)\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform=transform_resnet)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"Carregando o modelo ResNet-18 pré-treinado...\")\n",
        "\n",
        "model_transfer = models.resnet18(pretrained=True)\n",
        "\n",
        "for param in model_transfer.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_features = model_transfer.fc.in_features\n",
        "num_classes = len(train_dataset.classes)\n",
        "\n",
        "model_transfer.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, num_classes)\n",
        ")\n",
        "\n",
        "model_transfer = model_transfer.to(device)\n",
        "optimizer = optim.Adam(model_transfer.parameters(), lr=0.001, weight_decay=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "optimizer = optim.Adam(model_transfer.parameters(), lr=0.001, weight_decay=0.001)\n",
        "\n",
        "for eacho in range(num_epochs):\n",
        "  model_transfer.train()\n",
        "  running_loss = 0.0\n",
        "  for inputs, labels in train_loader:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outputs = model_transfer(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  epoch_loss = running_loss / len(train_loader)\n",
        "\n",
        "  model_transfer.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model_transfer(inputs)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  accuracy = 100 * correct / total\n",
        "  print(f'Epoch {eacho+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Acurácia Teste: {accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26Si8KA4usMC",
        "outputId": "3df25a36-1f7c-48bf-c9e4-92bac7d01384"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando o modelo ResNet-18 pré-treinado...\n",
            "Epoch 1/10, Loss: 1.6701, Acurácia Teste: 57.20%\n",
            "Epoch 2/10, Loss: 1.2962, Acurácia Teste: 71.60%\n",
            "Epoch 3/10, Loss: 1.1017, Acurácia Teste: 79.42%\n",
            "Epoch 4/10, Loss: 0.9629, Acurácia Teste: 79.01%\n",
            "Epoch 5/10, Loss: 0.9077, Acurácia Teste: 84.77%\n",
            "Epoch 6/10, Loss: 0.9093, Acurácia Teste: 83.74%\n",
            "Epoch 7/10, Loss: 0.8924, Acurácia Teste: 85.19%\n",
            "Epoch 8/10, Loss: 0.8732, Acurácia Teste: 82.92%\n",
            "Epoch 9/10, Loss: 0.8549, Acurácia Teste: 86.21%\n",
            "Epoch 10/10, Loss: 0.8319, Acurácia Teste: 86.42%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conlusão"
      ],
      "metadata": {
        "id": "e-dq0C3hDqqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na primeira parte,tentamos criar um rede neural do zero, ou seja, sem treinamento prévio, ajustando alguns parametros, o melhor resultado obtido foi de **64,8%** de acurácia.\n",
        "\n",
        "Depois utilizamos uma rede pré -treinada (ResNet-18) e assim foi possível chegar **88,8%** de acurácia.\n",
        "\n",
        "Ao final, tentamos melhorar ainda mais este resultado, com tecnicas de regularização e data augmatation. Aqui, quando utilizamos as tecnincas de regularização L2 e dropout, obtivemos um resultado ainda melhor de **89,51%** de acurácia. Porém utlizando as tecnicas de data augmatation, a acurácia foi diminuida."
      ],
      "metadata": {
        "id": "QrZiS5qbFRYt"
      }
    }
  ]
}